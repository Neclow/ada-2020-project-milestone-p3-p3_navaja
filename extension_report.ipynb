{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning functions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Downsampling function from imblearn, a library to manipulate imbalanced datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Data visualization functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the random seed for the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    seed = 20201106 # the deadline date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Step 1:_ Loading the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data dimensions: (7140, 285)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atwards</th>\n",
       "      <th>X</th>\n",
       "      <th>id</th>\n",
       "      <th>cid</th>\n",
       "      <th>cowcode</th>\n",
       "      <th>year</th>\n",
       "      <th>warstds</th>\n",
       "      <th>ptime</th>\n",
       "      <th>yrint</th>\n",
       "      <th>autonomy</th>\n",
       "      <th>...</th>\n",
       "      <th>decade1</th>\n",
       "      <th>decade2</th>\n",
       "      <th>decade3</th>\n",
       "      <th>decade4</th>\n",
       "      <th>independ</th>\n",
       "      <th>tip</th>\n",
       "      <th>anocracy</th>\n",
       "      <th>proxregc</th>\n",
       "      <th>sxpnew.2</th>\n",
       "      <th>sxpsq.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>1945</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143299</td>\n",
       "      <td>0.094095</td>\n",
       "      <td>0.094095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>1946</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094547</td>\n",
       "      <td>0.094547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095567</td>\n",
       "      <td>0.095567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101303</td>\n",
       "      <td>0.101303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>1949</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092107</td>\n",
       "      <td>0.092107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   atwards  X   id  cid  cowcode  year  warstds  ptime  yrint  autonomy  ...  \\\n",
       "1        0  1  1.0    1      700  1945        0     12      0  0.005151  ...   \n",
       "2        0  2  1.0    1      700  1946        0     24      1  0.000000  ...   \n",
       "3        0  3  1.0    1      700  1947        0     36      2  0.000000  ...   \n",
       "4        0  4  1.0    1      700  1948        0     48      3  0.000000  ...   \n",
       "5        0  5  1.0    1      700  1949        0     60      4  0.000000  ...   \n",
       "\n",
       "   decade1  decade2  decade3  decade4  independ   tip  anocracy  proxregc  \\\n",
       "1        0        0        0        0         1  17.0         0  0.143299   \n",
       "2        0        0        0        0         1  18.0         0  1.000000   \n",
       "3        0        0        0        0         1  19.0         0  1.000000   \n",
       "4        0        0        0        0         1  20.0         0  1.000000   \n",
       "5        0        0        0        0         1  21.0         0  1.000000   \n",
       "\n",
       "   sxpnew.2   sxpsq.2  \n",
       "1  0.094095  0.094095  \n",
       "2  0.094547  0.094547  \n",
       "3  0.095567  0.095567  \n",
       "4  0.101303  0.101303  \n",
       "5  0.092107  0.092107  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data \n",
    "df = pd.read_csv('data/SambnisImp.csv', index_col=0)\n",
    "\n",
    "# Print dimension information about the dataset\n",
    "print(f'Raw data dimensions: {df.shape}')\n",
    "\n",
    "# Print first 5 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data columns used by Muchlinski et al. (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns used in the Random Forests model from Muchlinski et al. (2016)\n",
    "\n",
    "rf_cols = [\"ager\", \"agexp\", \"anoc\", \"army85\", \n",
    "           \"autch98\", \"auto4\", \"autonomy\", \"avgnabo\",\n",
    "           \"centpol3\", \"coldwar\", \"decade1\", \"decade2\", \n",
    "           \"decade3\", \"decade4\", \"dem\", \"dem4\",\n",
    "           \"demch98\", \"dlang\", \"drace\", \"drel\", \n",
    "           \"durable\", \"ef\", \"ef2\", \"ehet\", \n",
    "           \"elfo\", \"elfo2\", \"etdo4590\", \"expgdp\", \n",
    "           \"exrec\", \"fedpol3\", \"fuelexp\", \"gdpgrowth\", \n",
    "           \"geo1\", \"geo2\", \"geo34\", \"geo57\",\n",
    "           \"geo69\", \"geo8\", \"illiteracy\", \"incumb\", \n",
    "           \"infant\", \"inst\", \"inst3\", \"life\",\n",
    "           \"lmtnest\", \"major\", \"manuexp\", \"milper\",\n",
    "           \"mirps0\", \"mirps1\", \"mirps2\", \"mirps3\", \n",
    "           \"nat_war\", \"ncontig\", \"nmgdp\", \"nmdp4_alt\",\n",
    "           \"numlang\", \"nwstate\", \"oil\", \"p4mchg\", \n",
    "           \"parcomp\", \"parreg\", \"part\", \"partfree\", \n",
    "           \"plural\", \"plurrel\", \"pol4\", \"pol4m\", \n",
    "           \"pol4sq\", \"polch98\", \"polcomp\", \"popdense\", \n",
    "           \"presi\", \"pri\", \"proxregc\", \"reg\", \n",
    "           \"regd4_alt\", \"relfrac\", \"seceduc\", \"second\",\n",
    "           \"semipol3\", \"sip2\", \"sxpnew\", \"sxpsq\",\n",
    "           \"tnatwar\", \"trade\", \"warhist\", \"xconst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, data_cols, label_col='warstds'):\n",
    "    '''\n",
    "    Select columns of interest from raw data\n",
    "    Separate data farme of interest into data matrix and labels\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Raw dataframe\n",
    "    data_cols: list\n",
    "        List of data columns to select the build the data matrix\n",
    "    label_col: str\n",
    "        Name of label column (ground truth)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    X: ndarray\n",
    "        The data matrix with the regressors\n",
    "    y: ndarray\n",
    "        A vector with the ground truth labels\n",
    "    '''\n",
    "    \n",
    "    X = df.loc[:, data_cols].to_numpy()\n",
    "    y = df.loc[:, label_col].to_numpy()\n",
    "    \n",
    "    print(f'Shape of dataset: {X.shape}')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = load_data(df, rf_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {'classification__n_estimators': [100],\n",
    "             'classification__max_features': ['auto', 'log2'],\n",
    "             'classification__random_state': [seed],\n",
    "             'classification__max_samples' : [2/3, 1],\n",
    "             'sampling__sampling_strategy': [0.125, 0.25, 0.5],\n",
    "             'sampling__random_state': [seed]\n",
    "            }\n",
    "\n",
    "params_xgb = {'classification__n_estimators': [100],\n",
    "              'classification__max_depth':[3, 6, 9], # Standard = 6, try 50% and 150% of default value\n",
    "              'classification__subsample': [2/3, 1], # Equivalent to max_samples \n",
    "              'classification__random_state': [seed],\n",
    "              'classification__booster': ['gbtree'], # Standard booster\n",
    "              'sampling__sampling_strategy': [0.125, 0.25, 0.5], # Different downsampling strategies (12.5%-25%-50% of war events)\n",
    "              'sampling__random_state': [seed]\n",
    "             }\n",
    "\n",
    "params_nn = {'classification__learning_rate': ['adaptive'],\n",
    "             'classification__alpha': [1e-4, 1e-3, 1e-2],\n",
    "             'classification__hidden_layer_sizes': [(X.shape[1],)*4, (X.shape[1],)*8], \n",
    "             'classification__learning_rate_init': [1e-3],\n",
    "             'classification__random_state': [seed],\n",
    "             'sampling__sampling_strategy': [0.125, 0.25, 0.5],\n",
    "             'sampling__random_state': [seed]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Pipeline([('sampling', RandomUnderSampler()),\n",
    "               ('classification', RandomForestClassifier())])\n",
    "\n",
    "xgb = Pipeline([('sampling', RandomUnderSampler()),\n",
    "                ('classification', XGBClassifier())])\n",
    "\n",
    "nn = Pipeline([('sampling', RandomUnderSampler()),\n",
    "               ('classification', MLPClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = []\n",
    "\n",
    "def fit_predict(X, y, clf, params, \n",
    "                test_size=0.15, n_splits=10, \n",
    "                scoring='average_precision', \n",
    "                seed=seed, normalize=False):\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    # Standardize features for MLP\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Apply Grid Search CV\n",
    "    grid = GridSearchCV(clf, params, cv=n_splits, scoring=scoring, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities and labels\n",
    "    y_pred_proba = grid.predict_proba(X_test)[:,1]\n",
    "    y_pred = grid.predict(X_test)\n",
    "    \n",
    "    # PR curve is more adapted to imbalanced datasets than ROC-AUC\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    # Get metrics\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return precision, recall, pr_auc, roc_auc, f1, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rf, r_rf, pr_auc_rf, roc_auc_rf, f1_rf, grid_rf = fit_predict(X, y, rf, params_rf)\n",
    "\n",
    "comparison_data['RF'] = {'PR': pr_auc_rf, 'ROC': roc_auc_rf, 'F1': f1_rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xgb, r_xgb, pr_auc_xgb, roc_auc_xgb, f1_xgb, grid_xgb= fit_predict(X, y, xgb, params_xgb)\n",
    "\n",
    "comparison_data['XGB'] = {'PR': pr_auc_xgb, 'ROC': roc_auc_xgb, 'F1': f1_xgb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nn, r_nn, pr_auc_nn, roc_auc_nn, f1_nn, grid_nn = fit_predict(X, y, nn, params_nn, normalize=True)\n",
    "\n",
    "comparison_data['NN'] = {'PR': pr_auc_nn, 'ROC': roc_auc_nn, 'F1': f1_nn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(comparison_data).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_rf, p_rf, 'k--', label=f'Random Forest {pr_auc_rf:.2f}', alpha=0.5)\n",
    "plt.plot(r_xgb, p_xgb, 'k--', label=f'XGBoost {pr_auc_xgb:.2f}')\n",
    "plt.plot(r_nn, p_nn, 'k', label=f'MLP {pr_auc_nn:.2f}', alpha=0.2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [\"Age in years of the current regime as classified by REG; ACLP\"\n",
    ",\"Agricultural raw materials exports as percentage of merchandise exports; WDI\"\n",
    ",\"Dummy: Anocracy=1\"\n",
    ", \"Size of government army in 1985\"\n",
    ", \"Autocracy annual change; Polity 98\"\n",
    ", \"Autocracy index from Polity IV\"\n",
    ", \"Country has de facto autonomous regions\"\n",
    ", \"Average SIP score of neighbors\"\n",
    ", \"Centralized state? (Polity III data plus updates for post-1994)\"\n",
    ", \"Code 1 for Cold War year - before 1990\"\n",
    ", \"Dummy : 1960s\"\n",
    ",\"Dummy : 1970s\"\n",
    ",\"Dummy : 1980s\"\n",
    ",\"Dummy : 1990s\"\n",
    ",\"Dummy: 1 for democracies and 0 for autocracies\"\n",
    ",\"Democracy index from Polity IV\"\n",
    ",\"Democracy annual change; Polity 98\"\n",
    ",\"Linguistic component of Ehet\"\n",
    ",\"Racial component of Ehet\"\n",
    ",\"Religious component of Ehet\"\n",
    ",\"Years since last regime transition/since 1949; Polity IV\"\n",
    ",\"Ethnic fractionalization index\"\n",
    ",\"Ef squared\"\n",
    ",\"Ethnic heterogeneity index\"\n",
    ",\"Ethnolinguistic diversity\"\n",
    ",\"Ethnolinguistic diversity, squared\"\n",
    ",\"Ethnic dominance measure\"\n",
    ",\"Exports of goods & services as %GDP; WDI data\"\n",
    ",\"Executive recruitment concept variable; Polity IV\"\n",
    ",\"Federal state? (Polity III data plus updates for post-1994)\"\n",
    ",\"Fuel and oil products exports as percentage of merchandise exports;WDI\"\n",
    ",\"Annual change in GDP, %\"\n",
    ",\"Region: Western Europe and the US\"\n",
    ",\"Region: Eastern Europe and Central Asia\"\n",
    ",\"Region: Middle East and North Africa\"\n",
    ",\"Region: South and East Asia and Oceania\"\n",
    ",\"Region: Latin America\"\n",
    ",\"Region: Sub-Saharan Africa\"\n",
    ",\"% adult population illiterate; WDI\"\n",
    ",\"Consolidation of incumbent advantage(Przeworski et al., 2000)\"\n",
    ",\"Infant mortality; WDI\"\n",
    ",\"0-dict; 1-parliam; 2-mixed dem; 3-pres dem (Przeworski et al., 2000)\"\n",
    ",\"Political instability; Whether Polity coded a change or 77 or 88 in previous three years\"\n",
    ",\"Life Expectancy at birth; WDI\"\n",
    ",\"Rough terrain\"\n",
    ",\"Majoritarian system\"\n",
    ",\"Manufactures exports as percentage of merchandise exports; WDI\"\n",
    ",\"Military manpower in thousands\"\n",
    ",\"Inconsistent polity (semi-democracy)\"\n",
    ",\"Caesaristic polity\"\n",
    ",\"Consistent autocracy\"\n",
    ",\"Consistent democracy\"\n",
    ",\"Whether a neighbor is at war in a given year.\"\n",
    ",\"Noncontiguous state\"\n",
    ",\"Neighborsâ€™ average ln(GDP per capita)\"\n",
    ",\"Neighborsâ€™ median polity (both land and water contiguity; using polity2)\"\n",
    ",\"Number of languages in Ethnologue\"\n",
    ",\"New state\"\n",
    ",\"Oil exports/GDP\"\n",
    ",\"Annual change in modified polity; Polity IV\"\n",
    ",\"Competitiveness of participation; non-elites; Polity IV\"\n",
    ",\"Regulation of participation; Polity IV\"\n",
    ",\"ln(share of population voting x oppositionâ€™s share of votes cast)\"\n",
    ",\"Partially free polity\"\n",
    ",\"Share of largest ethnic group\"\n",
    ",\"Size of largest confession\"\n",
    ",\"Polity index; Polity IV\"\n",
    ",\"Polity Index; Polity IV; 77 & 88 coded=0\"\n",
    ",\"Pol4 squared\"\n",
    ",\"Polity annual change; Polity98\"\n",
    ",\"Political competition: concept variable; Polity IV\"\n",
    ",\"Population density: people per square km; WDI\"\n",
    ",\"Presidential system\"\n",
    ",\"School enrollment, primary, %gross; WDI\"\n",
    ",\"2^(-durable/.5)\"\n",
    ",\"Dummy: 1 for dictatorships and 0 for democracies; ACLP\"\n",
    ",\"Median Regional polity (using polity2)\"\n",
    ",\"Religious fractionalization\"\n",
    ",\"School enrollment, secondary, %gross; WDI\"\n",
    ",\"Percent population in second largest group\"\n",
    ",\"Semi-federal state? (Polity III data plus updates for post-1994)\"\n",
    ",\"Continuous measure of democracy\"\n",
    ",\"Primary commodity exports/GDP\"\n",
    ",\"Primary commodity exports/GDP, squared\"\n",
    ",\"Total number of neighbors at war in a given year.\"\n",
    ",\"Trade as percept of GDP; in 1995 constant dollars\"\n",
    ",\"War in the country since 1945?\"\n",
    ",\"Executive constraints - operational independence of CE; Polity IV\"]\n",
    "\n",
    "\n",
    "zip_iterator = zip(rf_cols, var_names)\n",
    "names_dict = dict(zip_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_imp_score = grid.best_estimator_.named_steps['classification'].feature_importances_\n",
    "importance = pd.DataFrame(data = np.transpose([rf_cols, var_names,feat_imp_score]), columns=[\"variable shortcut\", \"variable name\", \"importance\"])\n",
    "importance = importance.sort_values(\"importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance = importance.astype({'importance': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x=\"importance\", y=\"variable name\", data=importance.iloc[:20,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from eli5 import explain_weights\n",
    "explain_weights(grid.best_estimator_.named_steps['classification'], top=50, feature_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nn = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=seed)\n",
    "y_pred = grid_nn.predict(X_test)\n",
    "\n",
    "MAE_base = mean_absolute_error(y_test,y_pred)\n",
    "print(MAE_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "n_turn = 200\n",
    "importance_NN = np.zeros((n_turn, np.shape(X_test)[1]))\n",
    "\n",
    "for i in range(n_turn):\n",
    "    X_test_shuff = np.copy(X_test)\n",
    "    np.random.shuffle(X_test_shuff,)\n",
    "    \n",
    "    for j in range(np.shape(X_test)[1]):\n",
    "        X_test_imp = np.copy(X_test)\n",
    "        X_test_imp[:,j] = X_test_shuff[:,j]\n",
    "        y_pred = grid_nn.predict(X_test_imp)\n",
    "        MAE_shuff = mean_absolute_error(y_test,y_pred)\n",
    "        importance_NN[i,j] = (MAE_shuff - MAE_base)/MAE_base*100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.abs(X_test-X_test_shuff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(importance_NN, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_imp_score = np.mean(importance_NN, axis=0)\n",
    "importance = pd.DataFrame(data = np.transpose([rf_cols, var_names,feat_imp_score]), columns=[\"variable shortcut\", \"variable name\", \"importance\"])\n",
    "importance = importance.astype({'importance': 'float'})\n",
    "importance = importance.sort_values(\"importance\",ascending=False)\n",
    "sns.stripplot(x=\"importance\", y=\"variable name\", data=importance.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class MLP(nn.Module):\n",
    "    def __init__(self, n_layers, in_, hidden, out_ = 1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.in_ = in_\n",
    "        self.hidden = hidden\n",
    "        self.out_ = out_\n",
    "        self.layers = self.create_layers()\n",
    "    \n",
    "    \n",
    "    def create_layers(self):\n",
    "        layers = [nn.Linear(self.in_, self.hidden), nn.ReLU()]\n",
    "        \n",
    "        for i in range(1, self.n_layers-1):\n",
    "            layers.extend([nn.Linear(self.hidden, self.hidden), nn.ReLU()])\n",
    "        \n",
    "        layers.extend([nn.Linear(self.hidden, self.out_), nn.Sigmoid()])\n",
    "        \n",
    "        return nn.Sequential(*layers)                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "xgb = MLPClassifier(hidden_layer_sizes=(88,88))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train = SMOTE(sampling_strategy=0.2,random_state=seed).fit_resample(X_train, y_train)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "fpr, tpr, _ = precision_recall_curve(y_test, xgb.predict_proba(X_test)[:,1])\n",
    "    \n",
    "f1_score(y_test, xgb.predict(X_test)), average_precision_score(y_test, xgb.predict_proba(X_test)[:,1])\n",
    "\n",
    "#confusion_matrix(y_test, xgb.predict(X_test))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by regions\n",
    "\n",
    "Here we split the original dataset into 6 world subregions: \n",
    "* Western Europe and the US (_occident_)\n",
    "* Eastern Europe and Central (_east_euro_)\n",
    "* Middle East and North Africa (_north_af_)\n",
    "* South East Asia and Oceania (_oceania_)\n",
    "* Latin America (_lat_america_)\n",
    "* Sub-Saharan Africa (_sub_saharan_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions \n",
      " for occident: (1136, 285) \n",
      " for eastern and central europe: (608, 285) \n",
      " for North Africa and Middle East: (966, 285) \n",
      " for South East Asia and Oceania: (1229, 285) \n",
      " for Latin America: (1341, 285) \n",
      " for Sub-Saharan Africa: (1694, 285)\n",
      "Total number of lines: 6974, number of lines in original data: 7140\n"
     ]
    }
   ],
   "source": [
    "df_occident = df[df['geo1']==1]\n",
    "df_east_euro = df[df['geo2']==1]\n",
    "df_north_af = df[df['geo34']==1]\n",
    "df_oceania = df[df['geo57']==1]\n",
    "df_lat_america = df[df['geo69']==1]\n",
    "df_sub_saharan = df[df['geo8']==1]\n",
    "\n",
    "# Print dimension information about the datasets\n",
    "print(f'Data dimensions \\n for occident: {df_occident.shape} \\n for eastern and central europe: {df_east_euro.shape} \\n for North Africa and Middle East: {df_north_af.shape} \\n for South East Asia and Oceania: {df_oceania.shape} \\n for Latin America: {df_lat_america.shape} \\n for Sub-Saharan Africa: {df_sub_saharan.shape}')\n",
    "print(f'Total number of lines: {sum([df_occident.shape[0], df_east_euro.shape[0], df_north_af.shape[0], df_oceania.shape[0], df_lat_america.shape[0], df_sub_saharan.shape[0]])}, number of lines in original data: {df.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
